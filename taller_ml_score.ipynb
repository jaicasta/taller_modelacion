{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from functions import *\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "import mlflow\n",
    "import optuna\n",
    "from optuna.samplers import RandomSampler, TPESampler\n",
    "import pickle\n",
    "\n",
    "from helper import Helper\n",
    "#Sparky\n",
    "from sparky_bc.sparky import Sparky"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = Sparky('lasalaza', 'IMPALA_PROD', hostname=\"sbmdeblze004.bancolombia.corp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constantes.\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "LIMITES_G = {\n",
    "    'G1': [0, 0.009],\n",
    "    'G2': [0.009, 0.017],\n",
    "    'G3': [0.017, 0.03],\n",
    "    'G4': [0.03, 0.05],\n",
    "    'G5': [0.05, 0.08],\n",
    "    'G6': [0.08, 0.12],\n",
    "    'G7': [0.12, 0.28],\n",
    "    'G8': [0.28, 1]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base de construcción del modelo.\n",
    "\n",
    "df_desembolsos = pd.read_csv('Dataset_curso_ML_python.csv')\n",
    "#df_desembolsos = sp.helper.obtener_dataframe(f\"SELECT * FROM resultados_bipa_vpr.score_orig_pasivos_17939_base_entrenamiento_v2\" )\n",
    "\n",
    "df_desembolsos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_desembolsos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_default_rate(df_desembolsos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables_predictoras = [c for c in df_desembolsos.columns if c not in ['id', 'tipo_doc', 'f_analisis', 'llave_sistema', 'segm', 'default']]\n",
    "len(variables_predictoras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creación de dos variables aleatorias de ruido\n",
    "\n",
    "df_desembolsos = df_desembolsos.assign(random_normal=np.random.normal(0, 1, size=(len(df_desembolsos.index))))\n",
    "df_desembolsos = df_desembolsos.assign(random_uniform=np.random.uniform(0, 1, size=(len(df_desembolsos.index))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Número de clientes y tasa de default por fecha de análisis\n",
    "\n",
    "df_fechas = df_desembolsos.groupby('f_analisis').agg(clientes=('id', 'count'), default=('default', 'sum')).reset_index()\n",
    "\n",
    "df_fechas['%_clientes'] = df_fechas['clientes'] / sum(df_fechas['clientes'])\n",
    "df_fechas['%_clientes_acum'] = df_fechas['%_clientes'].cumsum()\n",
    "df_fechas['TDO'] = df_fechas['default'] / df_fechas['clientes']\n",
    "\n",
    "df_fechas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [str(f) for f in df_fechas['f_analisis']]\n",
    "y = df_fechas['TDO']\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x, y)\n",
    "#ax.legend()\n",
    "\n",
    "ax.set_ylabel('Tasa de Default')\n",
    "ax.set_xlabel('Fecha de Análisis')\n",
    "\n",
    "ax.set_ylim([0.02, 0.2])\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "fig.set_figwidth(15)\n",
    "plt.grid(color='c', linestyle='--', linewidth=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Particionamiento de la base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_entrenamiento = df_desembolsos[df_desembolsos['f_analisis'] <= 201809]\n",
    "df_fuera_tiempo = df_desembolsos[df_desembolsos['f_analisis'] > 201809]\n",
    "\n",
    "print('Entrenamiento', df_entrenamiento.shape)\n",
    "print('Fuera de tiempo', df_fuera_tiempo.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = df_entrenamiento[variables_predictoras + ['random_uniform', 'random_normal']], df_entrenamiento['default']\n",
    "X_oot, y_oot = df_fuera_tiempo[variables_predictoras + ['random_uniform', 'random_normal']], df_fuera_tiempo['default']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Entrenamiento:', X_train.shape, 'Tasa Default:', sum(y_train) / len(y_train))\n",
    "print('Prueba', X_test.shape, 'Tasa Default:', sum(y_test) / len(y_test))\n",
    "print('Fuera de tiempo (OOT)', X_oot.shape, 'Tasa Default:', sum(y_oot) / len(y_oot))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Ensamble débil para depurar variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(max_depth=10, n_estimators=100, verbose=2, random_state=42)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importancias = dict(zip(X_train.columns, clf.feature_importances_))\n",
    "importancias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_importancias = pd.DataFrame(importancias.items(), columns=['variable', 'importancia'])\n",
    "df_importancias = df_importancias.sort_values(by='importancia', ascending=False)\n",
    "df_importancias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umbral = df_importancias[df_importancias['variable'].isin(['random_uniform', 'random_normal'])].iloc[0]['importancia']\n",
    "\n",
    "variables_predictoras = df_importancias[df_importancias['importancia'] > umbral]['variable'].tolist()\n",
    "len(variables_predictoras)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Búsqueda de Hiperparámetros (espacio amplio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = df_entrenamiento[variables_predictoras + ['random_uniform', 'random_normal']], df_entrenamiento['default']\n",
    "X_oot, y_oot = df_fuera_tiempo[variables_predictoras], df_fuera_tiempo['default']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    params = {\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.05, 0.8),\n",
    "        'max_iter': trial.suggest_int('max_iter', 50, 800),\n",
    "        'max_leaf_nodes': trial.suggest_int('max_leaf_nodes', 10, 80),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 30),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1000, 15000),\n",
    "        'l2_regularization': trial.suggest_float('l2_regularization', 0, 0.5),\n",
    "        'max_bins': trial.suggest_int('max_bins', 5, 255),\n",
    "        'validation_fraction': trial.suggest_float('validation_fraction', 0.05, 0.5)\n",
    "    }\n",
    "        \n",
    "    clf = HistGradientBoostingClassifier(random_state=SEED).set_params(**params)\n",
    "    clf.fit(X_train, y_train)\n",
    "        \n",
    "    y_proba_train = clf.predict_proba(X_train)[:,1]\n",
    "    y_proba_test = clf.predict_proba(X_test)[:,1]\n",
    "    \n",
    "    metrics_train = get_metrics(y_train, y_proba_train, [('G1', 'G6')], LIMITES_G, 'entr')\n",
    "    metrics_test = get_metrics(y_test, y_proba_test, [('G1', 'G6')], LIMITES_G, 'prueba')\n",
    "    \n",
    "    loss = metrics_train['n_en_rango_g1_g6_entr'] + metrics_test['n_en_rango_g1_g6_prueba']\n",
    "\n",
    "    with mlflow.start_run(nested=True):\n",
    "        mlflow.log_params(params)\n",
    "        mlflow.log_metrics(metrics_train)\n",
    "        mlflow.log_metrics(metrics_test)\n",
    "        mlflow.log_metric('loss', loss)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecución de la optimización con el espacio amplio.\n",
    "\n",
    "with mlflow.start_run() as run:\n",
    "    print(\"tags.mlflow.parentRunId = '\" + run.info.run_id + \"'\")\n",
    "    \n",
    "    sampler = RandomSampler(seed=10)\n",
    "    \n",
    "    study = optuna.create_study(sampler=sampler, direction='maximize')\n",
    "    study.optimize(objective, n_trials=5, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reto 1\n",
    "El modelo propuesto se entrena usando Histogram-based Gradient Boosting Classification Tree, se tiene la idea de implementar el modelo usando LightGBM, ¿Cómo podemos hacer esto? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mejores hiperparámetros del modelo.\n",
    "\n",
    "params = study.best_params\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probar el mejor modelo seleccionado.\n",
    "\n",
    "clf = HistGradientBoostingClassifier(random_state=SEED).set_params(**params)\n",
    "clf.fit(X_train, y_train)\n",
    "    \n",
    "y_proba_train = clf.predict_proba(X_train)[:,1]\n",
    "y_proba_test = clf.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tabla con TDO y % Clientes por G en entrenamiento.\n",
    "\n",
    "cumulative_gains_table(y_train, y_proba_train, LIMITES_G, percentage=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tabla con TDO y % Clientes por G en prueba.\n",
    "\n",
    "cumulative_gains_table(y_test, y_proba_test, LIMITES_G, percentage=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reto 2\n",
    "Mostrar en la tabla anterior dos columnas que indiquen la proporción de clientes buenos y malos por cada G sobre la distribución total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importancia de los hiperparámetros.\n",
    "\n",
    "optuna.visualization.plot_param_importances(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importancias del mejor modelo del espacio de búsqueda amplio.\n",
    "\n",
    "importances = permutation_importance(estimator=clf, X=X_train, y=y_train, n_repeats=5, n_jobs=-1, random_state=SEED)\n",
    "importances_dict = dict(zip(variables_predictoras + ['random_uniform', 'random_normal'], importances['importances_mean']))\n",
    "\n",
    "importances_df = pd.DataFrame(importances_dict.items(), columns=['feature', 'importance'])\n",
    "importances_df = importances_df.sort_values(by='importance', ascending=False)\n",
    "\n",
    "importances_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selección de variables más importantes que los dos ruidos aleatorios.\n",
    "\n",
    "threshold = importances_df[importances_df['feature'].isin(['random_uniform', 'random_normal'])].iloc[0]['importance']\n",
    "variables_predictoras = importances_df[importances_df['importance'] > threshold]['feature'].tolist()\n",
    "\n",
    "len(variables_predictoras)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Búsqueda de Hiperparámetros (espacio acotado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = df_entrenamiento[variables_predictoras + ['random_uniform', 'random_normal']], df_entrenamiento['default']\n",
    "X_oot, y_oot = df_fuera_tiempo[variables_predictoras], df_fuera_tiempo['default']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    params = {\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.05, 0.8),\n",
    "        'max_iter': trial.suggest_int('max_iter', 50, 800),\n",
    "        'max_leaf_nodes': trial.suggest_int('max_leaf_nodes', 10, 80),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 30),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1000, 15000),\n",
    "        'l2_regularization': trial.suggest_float('l2_regularization', 0, 0.5),\n",
    "        'max_bins': trial.suggest_int('max_bins', 5, 255),\n",
    "        'validation_fraction': trial.suggest_float('validation_fraction', 0.05, 0.5)\n",
    "    }\n",
    "        \n",
    "    clf = HistGradientBoostingClassifier(random_state=SEED).set_params(**params)\n",
    "    clf.fit(X_train, y_train)\n",
    "        \n",
    "    y_proba_train = clf.predict_proba(X_train)[:,1]\n",
    "    y_proba_test = clf.predict_proba(X_test)[:,1]\n",
    "    \n",
    "    metrics_train = get_metrics(y_train, y_proba_train, [('G1', 'G6')], LIMITES_G, 'entr')\n",
    "    metrics_test = get_metrics(y_test, y_proba_test, [('G1', 'G6')], LIMITES_G, 'prueba')\n",
    "    \n",
    "    loss = metrics_train['n_en_rango_g1_g6_entr'] + metrics_test['n_en_rango_g1_g6_prueba']\n",
    "\n",
    "    with mlflow.start_run(nested=True):\n",
    "        mlflow.log_params(params)\n",
    "        mlflow.log_metrics(metrics_train)\n",
    "        mlflow.log_metrics(metrics_test)\n",
    "        mlflow.log_metric('loss', loss)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecución de la optimización con el espacio acotado.\n",
    "\n",
    "with mlflow.start_run() as run:\n",
    "    print(\"tags.mlflow.parentRunId = '\" + run.info.run_id + \"'\")\n",
    "    \n",
    "    sampler = TPESampler(seed=10)  # Make the sampler behave in a deterministic way.\n",
    "    \n",
    "    study_2 = optuna.create_study(sampler=sampler, direction='maximize')\n",
    "    study_2.optimize(objective, n_trials=5, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mejores hiperparámetros (mejor modelo) de la optimización.\n",
    "\n",
    "params_2 = study_2.best_params\n",
    "params_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probar el mejor modelo seleccionado.\n",
    "\n",
    "clf = HistGradientBoostingClassifier(random_state=SEED).set_params(**params_2)\n",
    "clf.fit(X_train, y_train)\n",
    "    \n",
    "y_proba_train = clf.predict_proba(X_train)[:,1]\n",
    "y_proba_test = clf.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tabla con TDO y % Clientes por G en entrenamiento.\n",
    "\n",
    "cumulative_gains_table(y_train, y_proba_train, LIMITES_G, percentage=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tabla con TDO y % Clientes por G en prueba.\n",
    "\n",
    "cumulative_gains_table(y_test, y_proba_test, LIMITES_G, percentage=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importancia de los hiperparámetros.\n",
    "\n",
    "optuna.visualization.plot_param_importances(study_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importancias del mejor modelo del espacio de búsqueda acotado.\n",
    "\n",
    "importances = permutation_importance(estimator=clf, X=X_train, y=y_train, n_repeats=5, n_jobs=-1, random_state=SEED)\n",
    "importances_dict = dict(zip(variables_predictoras + ['random_uniform', 'random_normal'], importances['importances_mean']))\n",
    "\n",
    "importances_df = pd.DataFrame(importances_dict.items(), columns=['feature', 'importance'])\n",
    "importances_df = importances_df.sort_values(by='importance', ascending=False)\n",
    "\n",
    "importances_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selección de variables más importantes que los dos ruidos aleatorios.\n",
    "\n",
    "threshold = importances_df[importances_df['feature'].isin(['random_uniform', 'random_normal'])].iloc[0]['importance']\n",
    "variables_predictoras = importances_df[importances_df['importance'] > threshold]['feature'].tolist()\n",
    "\n",
    "len(variables_predictoras)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Mejor Modelo Seleccionado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = df_entrenamiento[variables_predictoras], df_entrenamiento['default']\n",
    "X_oot, y_oot = df_fuera_tiempo[variables_predictoras], df_fuera_tiempo['default']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo final.\n",
    "\n",
    "clf = HistGradientBoostingClassifier(random_state=SEED).set_params(**params_2)\n",
    "clf.fit(X_train, y_train)\n",
    "    \n",
    "y_proba_train = clf.predict_proba(X_train)[:,1]\n",
    "y_proba_test = clf.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tabla con TDO y % Clientes por G en entrenamiento.\n",
    "\n",
    "cumulative_gains_table(y_train, y_proba_train, LIMITES_G, percentage=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tabla con TDO y % Clientes por G en prueba.\n",
    "\n",
    "cumulative_gains_table(y_test, y_proba_test, LIMITES_G, percentage=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guadar PKL del modelo final.\n",
    "\n",
    "with open('modelo_score.pkl', \"wb\") as modelo_pkl:\n",
    "    pickle.dump(clf, modelo_pkl)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5.4 ('bancolombia_env_354')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f753396d9ec4ea6b3935a9baea2e105ec65b15ebc673ed011479faef62790c09"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
